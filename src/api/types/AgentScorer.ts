// This file was auto-generated by Fern from our API Definition.

import type * as Scout from "../index.js";

/**
 * Agent-based scorer using ephemeral agents.
 *
 * Uses Scout's ephemeral agent system to evaluate outputs. The agent can
 * reason about the evaluation, use tools, and return a composite score with
 * dimensional breakdown.
 *
 * IMPORTANT: The prompt_template should contain ONLY evaluation instructions.
 * DO NOT include JSON schema definitions - they are automatically generated
 * from the 'criteria' field. The system enforces this output structure:
 * {
 *     "score": 0.82,          // Composite score (0-1)
 *     "dimensions": {         // Scores for each criterion
 *         "helpfulness": 0.9,
 *         "accuracy": 0.7
 *     },
 *     "reasoning": "..."      // Explanation of scores
 * }
 *
 * Example (CORRECT - simple evaluation):
 *     scorer = AgentScorer(
 *         id="quality_agent",
 *         name="Response Quality",
 *         type="agent",
 *         prompt_template='''Evaluate the response quality.
 *
 *         Question: {{input}}
 *         Answer: {{output}}
 *         Expected: {{expected}}
 *
 *         Consider helpfulness and accuracy in your evaluation.''',
 *         model="claude-sonnet-4",
 *         criteria=["helpfulness", "accuracy"]  # Defines JSON dimensions
 *     )
 *
 * Example (CORRECT - with tools):
 *     scorer = AgentScorer(
 *         id="factual_accuracy",
 *         name="Factual Accuracy Agent",
 *         type="agent",
 *         prompt_template='''Check if the output is factually accurate.
 *         Query the knowledge base if needed to verify claims.
 *
 *         Output to verify: {{output}}''',
 *         model="claude-sonnet-4",
 *         criteria=["accuracy", "grounded"],
 *         tools=["scout_table_tools", "scout_document_tools"]
 *     )
 *
 * Example (WRONG - don't do this):
 *     scorer = AgentScorer(
 *         ...
 *         prompt_template='''Evaluate...
 *
 *         Return JSON: {"score": 0.9, "reasoning": "..."}''',  # ‚ùå NO!
 *         ...
 *     )
 */
export interface AgentScorer {
    type: "agent";
    /** Unique scorer identifier */
    id: string;
    /** Human-readable scorer name */
    name: string;
    /** What this scorer evaluates */
    description: string;
    /** Optional pass/fail threshold (0-1) */
    pass_threshold?: number;
    /** Categorization tags */
    tags?: string[];
    /** Prompt template with {input}, {output}, {expected} placeholders */
    prompt_template: string;
    /** Model to use for agent */
    model?: string;
    /** [DEPRECATED] Use score_fields instead. List of criterion names. */
    criteria?: string[];
    /** [DEPRECATED] Use score_fields with weight property instead. */
    dimension_weights?: Record<string, number | undefined>;
    /** Rich scoring dimension configurations with min/max/weight/description */
    score_fields?: Scout.ScoreField[];
    /** Scout tools agent can use (e.g., ['scout_table_tools']) */
    tools?: string[];
    /** Declared template variables. At execution time, validates that required variables are present in context. */
    variables?: Scout.TemplateVariable[];
}
