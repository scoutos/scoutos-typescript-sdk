// This file was auto-generated by Fern from our API Definition.

import type * as Scout from "../index.js";

/**
 * Evaluation configuration (reusable test suite specification).
 *
 * This represents the WHAT and HOW of an evaluation:
 * - WHAT: The target being evaluated (workflow, agent, tool, etc.)
 * - HOW: The test cases and execution settings
 *
 * This config can be:
 * 1. Saved as an Evaluation entity (reusable)
 * 2. Used directly for ephemeral runs (one-time)
 */
export interface EvaluationConfigInput {
    /** What to evaluate (workflow/agent/tool/prompt) */
    target: EvaluationConfigInput.Target;
    /** Test cases to run */
    test_cases: Scout.TestCase[];
    /** Registry of scorer definitions used in test cases (scorer_id -> Scorer dict) */
    scorer_registry?: Record<string, Record<string, unknown> | undefined>;
    /** Maximum concurrent test executions */
    max_concurrent?: number;
    /** Timeout per test in milliseconds */
    timeout_ms?: number;
    /** Stop evaluation after first test failure */
    stop_on_first_failure?: boolean;
    /** Tags for organization (model version, dataset, etc.) */
    tags?: string[];
    /** Additional custom metadata */
    metadata?: Record<string, unknown>;
}

export namespace EvaluationConfigInput {
    /**
     * What to evaluate (workflow/agent/tool/prompt)
     */
    export type Target = Scout.AgentTarget | Scout.PromptTarget | Scout.ToolTarget | Scout.WorkflowTarget;
}
