// This file was auto-generated by Fern from our API Definition.

/**
 * Aggregate metrics across all test cases
 */
export interface EvaluationMetrics {
    /** Percentage of tests passed (0-1) */
    pass_rate: number;
    /** Exact match accuracy (0-1) */
    accuracy: number;
    /** Average semantic similarity score (0-1) */
    avg_similarity_score?: number;
    /** Minimum similarity score across tests */
    min_similarity_score?: number;
    /** Maximum similarity score across tests */
    max_similarity_score?: number;
    /** Average correctness score from LLM judge */
    avg_correctness?: number;
    /** Average helpfulness score from LLM judge */
    avg_helpfulness?: number;
    /** Average coherence score from LLM judge */
    avg_coherence?: number;
    /** Scores for custom judge criteria */
    judge_scores?: Record<string, number | undefined>;
    /** Per-scorer aggregated metrics {scorer_id: {avg, min, max, pass_rate}} */
    scorer_metrics?: Record<string, Record<string, number | undefined> | undefined>;
    /** Average composite score across all tests (0-1) */
    avg_composite_score?: number;
    /** Number of test cases with pending human reviews */
    human_review_pending_count?: number;
    /** Average test latency */
    avg_latency_ms: number;
    /** Median test latency */
    median_latency_ms: number;
    /** 95th percentile latency */
    p95_latency_ms: number;
    /** Minimum test latency */
    min_latency_ms: number;
    /** Maximum test latency */
    max_latency_ms: number;
    /** Total tokens used across all tests */
    total_tokens?: number;
    /** Total cost in USD */
    total_cost_usd?: number;
    /** Number of tests passed */
    pass_count: number;
    /** Number of tests failed */
    fail_count: number;
    /** Number of tests with errors */
    error_count: number;
    /** Total number of tests */
    total_count: number;
}
